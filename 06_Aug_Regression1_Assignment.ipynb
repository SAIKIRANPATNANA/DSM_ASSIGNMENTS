{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "Answer: Simple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables: One variable, denoted x, is regarded as the predictor, explanatory, or independent variable. The other variable, denoted y, is regarded as the response, outcome, or dependent variable. For example, predicting a person's weight based on their height.\n",
    "\n",
    "Multiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. For example, predicting a person's weight based on their height, age, and diet.\n",
    "\n",
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "Answer: There are five key assumptions in linear regression:\n",
    "\n",
    "- Linear relationship: The relationship between the independent and dependent variables is linear.\n",
    "- Independence: The residuals are independent.\n",
    "- Homoscedasticity: The residuals have constant variance at every level of x.\n",
    "- Normality: The residuals of the model are normally distributed.\n",
    "- Lack of multicollinearity: The independent variables are not too highly correlated.\n",
    "\n",
    "These assumptions can be checked by using various plots like scatterplot (for linearity), residual vs fitted values plot (for homoscedasticity), Q-Q plot (for normality), etc. For checking multicollinearity, we can use Variance Inflation Factor (VIF) or correlation matrix.\n",
    "\n",
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "Answer: The intercept (often labeled as constant) is the point where the function crosses the y-axis. In terms of linear regression, it shows the point where the estimated regression line crosses the y-axis which means the value of the target variable if all the predictors are set to zero.\n",
    "\n",
    "The slope of the regression line (often labeled as the coefficient) shows how much the predicted response value increases/decreases when the particular predictor increases by one unit, holding all the other predictors constant. For example, in a regression model predicting price of a car based on its age, the slope will tell us how much the price of the car decreases when its age increases by one year.\n",
    "\n",
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "Answer: Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.\n",
    "\n",
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "Answer: Multiple linear regression is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. The goal of multiple linear regression is to model the linear relationship between the explanatory (independent) variables and response (dependent) variable. In contrast to simple linear regression, where a single predictor variable is used to predict the response, in multiple regression two or more predictors are used to predict the response.\n",
    "\n",
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "Answer: Multicollinearity occurs when independent variables in a regression model are correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results. Multicollinearity can be detected using Variance Inflation Factor (VIF) or correlation matrix. To address multicollinearity, we can do one of the following: remove some of the highly correlated independent variables, linearly combine the independent variables, such as adding them together, or use regularization methods.\n",
    "\n",
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "Answer: Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y. It differs from linear regression as it can model the relationship between x and y which are not linear.\n",
    "\n",
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "Answer: Polynomial regression models can fit data more closely than linear regression as they can model relationships between variables that are not linear. They can model curves, which makes them flexible to fit a wide range of data. However, they are more complex and hence computationally intensive. They are also prone to overfitting if the degree of polynomial is too high.\n",
    "\n",
    "Linear regression, on the other hand, is simple and computationally efficient. But, it may not provide a good fit to the data if the relationship is not linear.\n",
    "\n",
    "You would prefer to use polynomial regression when the relationship between variables is not linear and can be better represented by curves. For example, in predicting growth rate of organisms or depreciation of assets over time which are often curvilinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
