{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1)Desccibe the pucpose and benefits oj pooling in CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pooling is a critical feature of Convolutional Neural Networks. It's a form of non-linear down-sampling that condenses the image data extracted by the kernels/filters. The pooling layer serves several primary purposes:\n",
    "\n",
    "1. **Dimensionality Reduction**: It reduces the computational complexity by reducing the spatial dimensions (height, width) of the input volume for the next convolutional layer. However, the depth dimension remains unchanged.\n",
    "\n",
    "2. **Invariance to Small Translations**: A pooling layer also provides a form of translation invariance. This means that if the image shifts slightly, the pooled features will not change dramatically. This is useful because it allows the CNN to recognize the object regardless of its location in the image.\n",
    "\n",
    "3. **Preventing Overfitting**: By providing an abstracted form of the input image, pooling helps to prevent overfitting by creating an informational bottleneck, forcing the network to learn only the most important features.\n",
    "\n",
    "The most common types of pooling are:\n",
    "\n",
    "- **Max Pooling**: This method selects the maximum value from the region of the feature map covered by the filter. Therefore, it focuses on retaining the highest feature responses.\n",
    "\n",
    "- **Average Pooling**: This takes the average of the values from the region of the feature map covered by the filter. This leads to downscaling and loss of detailed information, but it provides a more holistic view of the features.\n",
    "\n",
    "- **Global Pooling**: This calculates the max or the average of an entire feature map and is usually used as an alternative to fully connected layers in CNN architectures.\n",
    "\n",
    "Remember, while pooling has many benefits, it is not always needed. The decision to include pooling layers depends on the specific requirements of your task, the complexity of the input data, and the specific architecture of your network. Modern CNN architectures sometimes use strided convolutions for downsampling instead of pooling layers, or use pooling layers sparingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2)Explain the difference between in minpooling and max pooling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Max Pooling**: Max pooling is a type of operation that extracts the maximum value from each region of a feature map that the pooling kernel covers. This process helps to retain the most activated features while discarding the non-activated or lesser-activated ones. Max pooling operation provides a form of translation invariance, meaning even if the input (image) shifts slightly, the output (pooled feature map) remains more or less the same.\n",
    "\n",
    "2. **Min Pooling**: Min pooling, on the other hand, would extract the minimum value from each region of a feature map that the pooling kernel covers. This operation is less commonly used than max pooling in practice. While max pooling aims to detect if a specific feature is present in various areas of the image, min pooling would do the opposite, that is, indicating the absence of a certain feature in various areas of the image.\n",
    "\n",
    "So, the primary difference is that max pooling takes the maximum value in a specific window, preserving the most significant feature, while min pooling takes the minimum value, focusing more on the absence of certain features. In most practical applications, max pooling is used due to its effectiveness in feature preservation and extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3)Discuss the concept of padding in CNN and its significance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding is an important concept in the context of Convolutional Neural Networks (CNNs). It refers to the process of adding extra artificial pixels around the border of the input image. The value of these added pixels is typically set to zero.\n",
    "\n",
    "There are several reasons why padding is used:\n",
    "\n",
    "1. **Preserving Spatial Dimensions**: Without padding, the size of the output feature map after convolution would be smaller than the input size due to the nature of the convolution operation. If we want the output volume to have the same spatial dimensions (width and height) as the original input volume, we can use padding.\n",
    "\n",
    "2. **Maintaining Information at the Borders**: Without padding, the pixels on the border of an image are used much less in the output than pixels in the center. The reason is that the convolution operation can't be applied to border pixels as many times as it can for the inner pixels (because border pixels have fewer neighboring pixels than those in the center). By adding padding, this issue is mitigated, and the convolution operation can equally involve all the pixels.\n",
    "\n",
    "3. **Control Over the Output Size**: Padding provides an additional level of control over the size of the output volumes, which can be crucial when designing a network architecture.\n",
    "\n",
    "There are two primary types of padding:\n",
    "\n",
    "- **Valid Padding (No Padding)**: In this case, the convolutional operation is only applied to the input pixels, not taking into account any padding (extra zeroes). As a result, the output dimensions are reduced.\n",
    "\n",
    "- **Same Padding**: This padding technique adds sufficient padding to the input matrix so that the output dimensions match the input dimensions. The term \"same\" refers to maintaining the same width and height dimensions.\n",
    "\n",
    "Remember, while padding can be useful, it's not always necessary. The decision to use padding depends on the specific problem, the complexity of the input data, and the specific architecture of your CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4) Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe you're asking to compare and contrast \"zero-padding\" and \"valid-padding\" in terms of their effects on the output feature map size in Convolutional Neural Networks (CNNs). Let's dive in:\n",
    "\n",
    "1. **Zero-padding (Same padding)**: Zero-padding involves adding extra rows and/or columns filled with zeros to the input data. The primary purpose of zero-padding is to preserve the spatial dimensions (width and height) of the input through the convolutional layer. With \"same\" padding (another name for zero-padding in this context), the spatial dimensions of the output feature map are the same as the input volume, assuming the convolutional layer uses a stride of 1. If you were to increase the stride, the dimensions would decrease.\n",
    "\n",
    "2. **Valid-padding (No padding)**: In valid-padding, no extra padding is added to the input data. Thus, the convolutional operation can only take place where the kernel/filter fits squarely within the input data. This implies that the spatial dimensions (width and height) of the output feature map are smaller than the input volume. The exact size of the reduction depends on the size of the kernel/filter.\n",
    "\n",
    "So, the key difference between zero-padding (same padding) and valid-padding (no padding) lies in how they handle the spatial dimensions of the input data through the convolutional layer:\n",
    "\n",
    "- Zero-padding ensures the output feature map has the same width and height as the input by adding extra zero-value pixels around the input. \n",
    "- Valid-padding, on the other hand, does not add any padding, resulting in an output feature map that is smaller than the input.\n",
    "\n",
    "In both cases, the depth of the output feature map is determined by the number of filters in the convolutional layer, not by the padding method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5)Provide a brief overview of LeNet-5 architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5 is a pioneering Convolutional Neural Network (CNN) model designed by Yann LeCun, Leon Bottou, Yosuha Bengio, and Patrick Haffner in 1998. It was used for digit recognition tasks, such as reading zip codes, digits in bank checks, etc.\n",
    "\n",
    "Here's a brief overview of the architecture:\n",
    "\n",
    "1. **Input Layer**: The input for LeNet-5 is a 32x32 grayscale image.\n",
    "\n",
    "2. **C1 - Convolutional Layer**: The first layer C1 is a convolutional layer with six (5x5) filters, resulting in output dimensions of 28x28x6.\n",
    "\n",
    "3. **S2 - Subsampling (Pooling) Layer**: The second layer S2 is a subsampling (or average pooling) layer, which reduces each 28x28 feature map to 14x14. It uses a 2x2 kernel, resulting in output dimensions of 14x14x6.\n",
    "\n",
    "4. **C3 - Convolutional Layer**: The third layer C3 is a convolutional layer with sixteen (5x5) filters, applied to subsets of the previous layers' feature maps, resulting in output dimensions of 10x10x16.\n",
    "\n",
    "5. **S4 - Subsampling (Pooling) Layer**: The fourth layer S4 is another subsampling (or average pooling) layer. It reduces each 10x10 feature map to 5x5, using a 2x2 kernel, resulting in output dimensions of 5x5x16.\n",
    "\n",
    "6. **C5 - Convolutional Layer**: The fifth layer C5 is a convolutional layer with 120 (5x5) filters. Due to the dimensions of the previous layer, this operation effectively functions as a fully connected layer, yielding 120 outputs.\n",
    "\n",
    "7. **F6 - Fully Connected Layer**: The sixth layer F6 is a fully connected layer with 84 neurons.\n",
    "\n",
    "8. **Output Layer**: The output layer consists of 10 neurons (in the context of digit recognition - for 10 digits), each neuron representing one digit class (0-9). A Radial Basis Function (RBF) neuron is used in the output layer.\n",
    "\n",
    "Each neuron in the LeNet-5 model includes a trainable bias. The activation function used is a sigmoid/tanh function, except in the output layer, where a Radial Basis Function is used.\n",
    "\n",
    "It's important to note that the LeNet-5 architecture was designed in the late 90s, before the advent of modern activation functions like ReLU and advanced techniques like Dropout or Batch Normalization. The specific design decisions made in the LeNet-5 architecture reflect the computational constraints and understanding of neural networks at the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6) Desccibe the key components of LeNet-5 and therc respective purposes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5, a classic Convolutional Neural Network (CNN) model, consists of several key components, each serving a specific purpose in the network's functionality. Here's a description of each component and its respective role:\n",
    "\n",
    "1. **Input Layer**: The input layer of LeNet-5 accepts grayscale images of size 32x32. The purpose of the input layer is to provide the initial data or features for the subsequent layers of the network.\n",
    "\n",
    "2. **Convolutional Layers (C1, C3, C5)**: These are the primary feature extractors in the network. Convolutional layers apply a series of filters to the input, helping to identify patterns or features such as edges, corners, etc. LeNet-5 has three convolutional layers:\n",
    "\n",
    "   - C1 has six (5x5) filters.\n",
    "   - C3 has sixteen (5x5) filters, applied on subsets of S2's feature maps.\n",
    "   - C5 has 120 (5x5) filters, effectively acting as a fully connected layer.\n",
    "\n",
    "3. **Pooling (Subsampling) Layers (S2, S4)**: These layers perform down-sampling operations to reduce the spatial dimensions (height, width) of the input volume, thus reducing computational complexity. They also provide a form of translation invariance, meaning that the network can still recognize a feature even if it's slightly shifted. LeNet-5 uses average pooling in its two pooling layers:\n",
    "\n",
    "   - S2 reduces each of C1's 28x28 feature map to 14x14.\n",
    "   - S4 reduces each of C3's 10x10 feature map to 5x5.\n",
    "\n",
    "4. **Fully Connected Layer (F6)**: After several convolutional and pooling layers, the high-level reasoning in the neural network occurs in the fully connected layer. Neurons in a fully connected layer have full connections to all activations in the previous layer. In LeNet-5, F6 has 84 neurons.\n",
    "\n",
    "5. **Output Layer**: The final layer of LeNet-5 is a softmax classifier consisting of 10 neurons (for the task of digit recognition). Each neuron represents one digit class (0-9), and the network's prediction is the digit corresponding to the most activated neuron.\n",
    "\n",
    "6. **Activation Function**: In LeNet-5, sigmoid/tanh activation functions are used to add non-linearity to the network, enabling it to learn complex patterns. The output layer uses a Radial Basis Function (RBF) neuron.\n",
    "\n",
    "7. **Trainable Parameters (Weights and Biases)**: These are the elements that the network learns during training. The network adjusts these parameters to minimize the difference between its predictions and actual values.\n",
    "\n",
    "Remember, the design of LeNet-5 was constrained by the computational resources available in the 1990s. Modern networks often have many more layers, use different types of layers (like normalization or dropout layers), and take advantage of advanced activation functions like ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7) Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5, developed by Yann LeCun and others in 1998, was a pioneering model in the field of Convolutional Neural Networks (CNNs). It was specifically designed for digit recognition tasks and had several advantages in its time:\n",
    "\n",
    "**Advantages of LeNet-5:**\n",
    "\n",
    "1. **Effectiveness**: LeNet-5 proved very effective for the task it was designed for, which is recognizing handwritten digits in images. It was used by banks to recognize numbers in checks and had a very high accuracy rate.\n",
    "\n",
    "2. **Architectural Foundation**: The architecture of LeNet-5 laid the groundwork for future CNNs. It introduced key components that are still used today, such as convolutional layers, subsampling (pooling) layers, and fully connected layers.\n",
    "\n",
    "3. **Size**: LeNet-5 is not a very deep network, which means it has fewer parameters than some later networks. This makes it relatively quick and easy to train, which can be an advantage in some situations.\n",
    "\n",
    "However, like all models, LeNet-5 also has limitations, especially when we view it in a modern context:\n",
    "\n",
    "**Limitations of LeNet-5:**\n",
    "\n",
    "1. **Simplicity**: LeNet-5 is a relatively simple model, with only a few layers. This simplicity means it might not perform as well as more complex models on more challenging tasks.\n",
    "\n",
    "2. **Limited Input Size**: LeNet-5 was designed to work with small grayscale images (32x32 pixels). This means it might not work as well with larger images or with color images without significant modification.\n",
    "\n",
    "3. **Activation Functions**: LeNet-5 uses sigmoid and tanh activation functions, which can cause the vanishing gradient problem, making the network harder to train effectively. Modern networks typically use the ReLU activation function to avoid this issue.\n",
    "\n",
    "4. **Lack of Modern Techniques**: LeNet-5 predates many techniques that are commonly used in modern networks to improve performance and reduce overfitting, such as dropout, batch normalization, or advanced regularization techniques.\n",
    "\n",
    "In conclusion, while LeNet-5 was a groundbreaking model in its time and continues to be effective for certain tasks, it is not as sophisticated or flexible as many modern CNNs. For complex image classification tasks, more recent architectures like AlexNet, VGG, or ResNet are generally more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8) Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 24, 24, 6)         156       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 12, 12, 6)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 16)          2416      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 16)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               30840     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44426 (173.54 KB)\n",
      "Trainable params: 44426 (173.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "425/430 [============================>.] - ETA: 0s - loss: 0.3079 - accuracy: 0.9067\n",
      "Epoch 1: val_loss improved from inf to 0.09060, saving model to lenet.architect.checkpoint.best.hdf5\n",
      "430/430 [==============================] - 6s 10ms/step - loss: 0.3055 - accuracy: 0.9074 - val_loss: 0.0906 - val_accuracy: 0.9746\n",
      "Epoch 2/10\n",
      "430/430 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9739\n",
      "Epoch 2: val_loss improved from 0.09060 to 0.06381, saving model to lenet.architect.checkpoint.best.hdf5\n",
      "430/430 [==============================] - 4s 10ms/step - loss: 0.0854 - accuracy: 0.9739 - val_loss: 0.0638 - val_accuracy: 0.9830\n",
      "Epoch 3/10\n",
      "428/430 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9812\n",
      "Epoch 3: val_loss improved from 0.06381 to 0.05729, saving model to lenet.architect.checkpoint.best.hdf5\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.0618 - accuracy: 0.9812 - val_loss: 0.0573 - val_accuracy: 0.9854\n",
      "Epoch 4/10\n",
      "425/430 [============================>.] - ETA: 0s - loss: 0.0513 - accuracy: 0.9839\n",
      "Epoch 4: val_loss improved from 0.05729 to 0.05204, saving model to lenet.architect.checkpoint.best.hdf5\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.0510 - accuracy: 0.9840 - val_loss: 0.0520 - val_accuracy: 0.9834\n",
      "Epoch 5/10\n",
      "430/430 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9867\n",
      "Epoch 5: val_loss did not improve from 0.05204\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.0428 - accuracy: 0.9867 - val_loss: 0.0533 - val_accuracy: 0.9860\n",
      "Epoch 6/10\n",
      "430/430 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9877\n",
      "Epoch 6: val_loss did not improve from 0.05204\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.0381 - accuracy: 0.9877 - val_loss: 0.0537 - val_accuracy: 0.9858\n",
      "Epoch 7/10\n",
      "428/430 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9899\n",
      "Epoch 7: val_loss improved from 0.05204 to 0.04032, saving model to lenet.architect.checkpoint.best.hdf5\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.0313 - accuracy: 0.9899 - val_loss: 0.0403 - val_accuracy: 0.9890\n",
      "Epoch 8/10\n",
      "430/430 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9907\n",
      "Epoch 8: val_loss improved from 0.04032 to 0.03946, saving model to lenet.architect.checkpoint.best.hdf5\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.0285 - accuracy: 0.9907 - val_loss: 0.0395 - val_accuracy: 0.9880\n",
      "Epoch 9/10\n",
      "424/430 [============================>.] - ETA: 0s - loss: 0.0239 - accuracy: 0.9926\n",
      "Epoch 9: val_loss did not improve from 0.03946\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.0242 - accuracy: 0.9925 - val_loss: 0.0447 - val_accuracy: 0.9876\n",
      "Epoch 10/10\n",
      "428/430 [============================>.] - ETA: 0s - loss: 0.0214 - accuracy: 0.9934\n",
      "Epoch 10: val_loss did not improve from 0.03946\n",
      "430/430 [==============================] - 4s 9ms/step - loss: 0.0214 - accuracy: 0.9934 - val_loss: 0.0440 - val_accuracy: 0.9888\n",
      "       loss  accuracy  val_loss  val_accuracy\n",
      "0  0.305544  0.907436  0.090598        0.9746\n",
      "1  0.085404  0.973945  0.063813        0.9830\n",
      "2  0.061827  0.981200  0.057293        0.9854\n",
      "3  0.051004  0.984018  0.052042        0.9834\n",
      "4  0.042795  0.986655  0.053266        0.9860\n",
      "5  0.038079  0.987709  0.053726        0.9858\n",
      "6  0.031268  0.989909  0.040320        0.9890\n",
      "7  0.028474  0.990691  0.039463        0.9880\n",
      "8  0.024230  0.992527  0.044707        0.9876\n",
      "9  0.021382  0.993364  0.044032        0.9888\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0344 - accuracy: 0.9899\n",
      "Accuracy: 98.989999294281\n"
     ]
    }
   ],
   "source": [
    "# General Model Implementaton with ANN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings as warn\n",
    "warn.filterwarnings('ignore')\n",
    "from tensorflow.keras.datasets import  mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train_full,y_train_full),(x_test,y_test) = mnist.load_data()\n",
    "x_train_full.reshape(len(x_train_full),28,28,1)\n",
    "x_test.reshape(len(x_test),28,28,1)\n",
    "y_train_full = to_categorical(y_train_full,num_classes=10)\n",
    "y_test = to_categorical(y_test,num_classes=10)\n",
    "x_train_full = x_train_full.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n",
    "x_train = x_train_full[5000:]\n",
    "x_valid = x_train_full[:5000]\n",
    "y_train = y_train_full[5000:]\n",
    "y_valid = y_train_full[:5000]\n",
    "Layers = [tf.keras.layers.Conv2D(6,kernel_size=(5,5),activation='relu',input_shape=(28,28,1)),\n",
    "          tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "          tf.keras.layers.Conv2D(16,kernel_size=(5,5),activation='relu'),\n",
    "          tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          tf.keras.layers.Dense(120,activation='relu'),\n",
    "          tf.keras.layers.Dense(84,activation='relu'),\n",
    "          tf.keras.layers.Dense(10,activation='softmax')]\n",
    "model_clf = tf.keras.Sequential(Layers)\n",
    "print(model_clf.summary())\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='lenet.architect.checkpoint.best.hdf5',save_best_only=True,verbose=1)\n",
    "model_clf.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history = model_clf.fit(x_train,y_train,validation_data=[x_valid,y_valid],batch_size=128,epochs=10,callbacks=[checkpointer],verbose=1,shuffle=True)\n",
    "print(pd.DataFrame(history.history))\n",
    "model_clf.load_weights('lenet.architect.checkpoint.best.hdf5')\n",
    "scores = model_clf.evaluate(x_test,y_test)\n",
    "print('Accuracy:',scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 12:24:14.215708: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-07 12:24:14.773079: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-07 12:24:14.776045: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-07 12:24:16.693685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 24, 24, 6)         156       \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 12, 12, 6)         0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 16)          2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 4, 4, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               30840     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44426 (173.54 KB)\n",
      "Trainable params: 44426 (173.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "469/469 [==============================] - 7s 12ms/step - loss: 0.3561 - accuracy: 0.8975 - val_loss: 0.1603 - val_accuracy: 0.9516\n",
      "Epoch 2/2\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1332 - accuracy: 0.9604 - val_loss: 0.1053 - val_accuracy: 0.9687\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1053 - accuracy: 0.9687\n",
      "Test Loss: 0.10530511289834976\n",
      "Test accuracy: 0.9686999917030334\n"
     ]
    }
   ],
   "source": [
    "#Implementaion of Lenet Architecture:\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(6, kernel_size = (5,5), padding = 'valid', activation='tanh', input_shape = (28,28,1)))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "model.add(Conv2D(16, kernel_size = (5,5), padding = 'valid', activation='tanh'))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='tanh'))\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "print(model.summary())\n",
    "model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=2, verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9)Present an overview of the AlexNet architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet is a deep learning model that significantly influenced the field of computer vision. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, it won the 2012 ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) competition by a significant margin. \n",
    "\n",
    "Here's a brief overview of its architecture:\n",
    "\n",
    "1. **Input Layer**: AlexNet accepts color images of size 224x224 pixels as its input. The images are passed through various layers for processing.\n",
    "\n",
    "2. **Convolutional Layers**: AlexNet contains five convolutional layers. These layers use a series of filters to detect a variety of features across the input images.\n",
    "\n",
    "    - **First Convolutional Layer (C1)**: The first layer uses 96 kernels of size 11x11 with a stride of 4. The output of this layer is then passed through a ReLU activation function for non-linearity. After that, a max pooling operation is performed, and then local response normalization (LRN) is applied.\n",
    "\n",
    "    - **Second Convolutional Layer (C2)**: The second layer uses 256 kernels of size 5x5. Again, ReLU activation is used, followed by max pooling and LRN.\n",
    "\n",
    "    - **Third, Fourth, and Fifth Convolutional Layers (C3-C5)**: These layers are connected without any pooling or normalization in between. The third layer has 384 kernels of size 3x3, the fourth layer has 384 kernels of size 3x3, and the fifth layer has 256 kernels of size 3x3. After the fifth layer, max pooling is performed.\n",
    "\n",
    "3. **Fully Connected Layers**: After the convolutional layers, AlexNet has three fully connected layers. The first two have 4096 nodes each, and the last one has 1000 nodes, corresponding to 1000 ImageNet classes.\n",
    "\n",
    "    - **First Fully Connected Layer (F6)**: The output from the previous layer is flattened and connected to this layer, which has 4096 neurons. It's followed by a ReLU activation function.\n",
    "\n",
    "    - **Second Fully Connected Layer (F7)**: This layer is also made up of 4096 neurons and applies a ReLU activation function.\n",
    "\n",
    "    - **Third Fully Connected Layer (F8)**: This layer has 1000 neurons, corresponding to the 1000 classes in the ImageNet dataset.\n",
    "\n",
    "4. **Output Layer**: The final layer uses a softmax function to output the model's predicted probabilities for each class.\n",
    "\n",
    "AlexNet uses ReLU as its activation function, which helps mitigate the vanishing gradient problem seen in deep neural networks. Furthermore, it applies dropout in the first two fully connected layers to prevent overfitting. This architecture was the first to demonstrate the feasibility and performance of deep convolutional neural networks, and it inspired the development of more complex architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10)Explain the architectural innovations introduced in AlexNet that contcibuted to its breakthrough performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet was indeed a breakthrough and brought several key innovations in the design and training of Convolutional Neural Networks (CNNs). Below are some of the significant advancements introduced with AlexNet:\n",
    "\n",
    "1. **Deep Architecture**: AlexNet demonstrated that deeper and larger neural networks could achieve superior performance. With eight layers (five convolutional and three fully connected), AlexNet was substantially deeper than previous models.\n",
    "\n",
    "2. **ReLU Activation Function**: Prior to AlexNet, most neural networks used sigmoid or tanh activation functions. AlexNet popularized the use of the Rectified Linear Unit (ReLU) activation function, which helped it train much faster. ReLU helps to alleviate the vanishing gradient problem, a common issue faced during the training of deep neural networks.\n",
    "\n",
    "3. **Use of Dropout**: AlexNet was one of the first models to implement dropout layers. Dropout is a regularization technique that helps prevent overfitting in neural networks. During training, dropout randomly sets a fraction of input units to 0 at each update, which helps to prevent overfitting.\n",
    "\n",
    "4. **Data Augmentation**: AlexNet introduced the idea of data augmentation, that is, artificially increasing the size of the training set using label-preserving transformations. For instance, they used image translations, horizontal reflections, and RGB color shifts. This helped the model generalize better and reduced overfitting.\n",
    "\n",
    "5. **GPU Training**: AlexNet's training was performed on two Nvidia GTX 580 GPUs, which allowed the network to handle its large size and computational complexity. The model was specifically designed to take advantage of parallel computation on GPUs, splitting the layers across two GPUs.\n",
    "\n",
    "6. **Local Response Normalization (LRN)**: AlexNet introduced a technique called \"Local Response Normalization,\" where the activations are scaled in such a way that higher values are suppressed and lower values are amplified. This method aims to promote lateral inhibition, a concept in neurobiology.\n",
    "\n",
    "7. **Overlapping Pooling**: AlexNet used pooling layers with a stride less than the size of the pooling kernel, leading to overlapping pooling. This was found to reduce the error rate compared to non-overlapping pooling.\n",
    "\n",
    "These innovations contributed significantly to AlexNet's groundbreaking performance on the ImageNet challenge and paved the way for future developments in the field of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11)Discuss the role of convolutional layecs, pooling layers, and fully connected layers in AlexNet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the AlexNet architecture, convolutional layers, pooling layers, and fully connected layers each play crucial roles in the overall functioning of the network:\n",
    "\n",
    "1. **Convolutional Layers**: The primary role of convolutional layers is to detect local conjunctions of features from the previous layer. The convolutional layers of AlexNet use a series of learned filters that are convolved across the height and width of the input volume. These convolutional operations detect various features, such as edges, corners, and color blobs, from the input images. AlexNet uses five convolutional layers of varying filter sizes and numbers.\n",
    "\n",
    "2. **Pooling Layers**: Pooling layers follow the first, second, and fifth convolutional layers in the AlexNet architecture. These layers reduce the spatial size of the representation, both to reduce the amount of parameters and computation in the network and to provide a form of translational invariance. The pooling layers in AlexNet perform max pooling, where the maximum value in each patch of the feature map is taken. \n",
    "\n",
    "3. **Fully Connected Layers**: Following the convolutional and pooling layers, AlexNet has three fully connected layers. These layers perform high-level reasoning in the network. Each neuron in these layers is connected to all the numbers in the previous volume. They combine the features learned by the convolutional layers across the entire image to identify the larger patterns. The last fully connected layer has 1000 neurons (for the 1000 ImageNet classes), and the output of the network is the class that receives the highest score.\n",
    "\n",
    "These layers work together to process, transform, and interpret the input data, from detecting simple features in the convolutional layers, reducing dimensionality in the pooling layers, to making final predictions in the fully connected layers. It's the combination of these different layer types that allow AlexNet, and convolutional neural networks in general, to learn from image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12) Implement AlexNet using a deep learning network of your choice and evaluate its performance on a dataset of your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import tensorflow as tf\\nimport  tflearn.datasets.oxflower17 as pushpa\\nx,y = pushpa.load_data()\\nx = x.astype('float32')/255\\nfrom tensorflow.keras.utils import to_categorical\\nimport numpy as np\\ny = to_categorical(y,num_classes=len(np.unique(y)))\\nx_train = x[:1000]\\nx_test = x[1000:]\\ny_train = y[:1000]\\ny_test = y[1000:]\\nLayers = [\\n    tf.keras.layers.Conv2D(filters=96,kernel_size=(11,11),strides=(4,4),padding='valid',input_shape=(224,224,3)),\\n    tf.keras.layers.Activation('relu'),\\n    tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2)),\\n    tf.keras.layers.BatchNormalization(),\\n    tf.keras.layers.Conv2D(filters=256,kernel_size=(5,5),strides=(1,1),padding='same'),\\n    tf.keras.layers.Activation('relu'),\\n    tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2)),\\n    tf.keras.layers.BatchNormalization(),\\n    tf.keras.layers.Conv2D(filters=384,kernel_size=(3,3),strides=(1,1),padding='valid'),\\n    tf.keras.layers.Activation('relu'),\\n    tf.keras.layers.BatchNormalization(),\\n    tf.keras.layers.Conv2D(filters=384,kernel_size=(3,3),strides=(1,1),padding='valid'),\\n    tf.keras.layers.Activation('relu'),\\n    tf.keras.layers.BatchNormalization(),\\n    tf.keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='valid'),\\n    tf.keras.layers.Activation('relu'),\\n    tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2)),\\n    tf.keras.layers.BatchNormalization(),\\n    tf.keras.layers.Flatten(),\\n    tf.keras.layers.Dense(4096,input_shape=(224*224*3,)),\\n    tf.keras.layers.Dropout(0.4),\\n    tf.keras.layers.BatchNormalization(),\\n    tf.keras.layers.Flatten(),\\n    tf.keras.layers.Dense(4096),\\n    tf.keras.layers.Dropout(0.4),\\n    tf.keras.layers.BatchNormalization(),\\n    tf.keras.layers.Dense(17),\\n    tf.keras.layers.Activation('softmax')]\\nmodel_clf = tf.keras.models.Sequential(Layers)\\nprint(model_clf.summary())\\nmodel_clf.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\\nhistory = model_clf.fit(x_train,y_train,epochs=10,verbose=1,batch_size=64,shuffle=True)\\nimport pandas as pd\\nprint(pd.DataFrame(history.history))\\nprint('Test_Accuracy: ',model_clf.evaluate(x_test,y_test)[1])  \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import tensorflow as tf\n",
    "import  tflearn.datasets.oxflower17 as pushpa\n",
    "x,y = pushpa.load_data()\n",
    "x = x.astype('float32')/255\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "y = to_categorical(y,num_classes=len(np.unique(y)))\n",
    "x_train = x[:1000]\n",
    "x_test = x[1000:]\n",
    "y_train = y[:1000]\n",
    "y_test = y[1000:]\n",
    "Layers = [\n",
    "    tf.keras.layers.Conv2D(filters=96,kernel_size=(11,11),strides=(4,4),padding='valid',input_shape=(224,224,3)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=256,kernel_size=(5,5),strides=(1,1),padding='same'),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=384,kernel_size=(3,3),strides=(1,1),padding='valid'),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=384,kernel_size=(3,3),strides=(1,1),padding='valid'),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='valid'),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4096,input_shape=(224*224*3,)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4096),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(17),\n",
    "    tf.keras.layers.Activation('softmax')]\n",
    "model_clf = tf.keras.models.Sequential(Layers)\n",
    "print(model_clf.summary())\n",
    "model_clf.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "history = model_clf.fit(x_train,y_train,epochs=10,verbose=1,batch_size=64,shuffle=True)\n",
    "import pandas as pd\n",
    "print(pd.DataFrame(history.history))\n",
    "print('Test_Accuracy: ',model_clf.evaluate(x_test,y_test)[1])  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apologies for not showing the output because of some technical issue with the tensorflow and not enougn space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
