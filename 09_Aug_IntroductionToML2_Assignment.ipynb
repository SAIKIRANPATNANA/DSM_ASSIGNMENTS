{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Define overfitting and underfitting in machine learning.**\n",
    "\n",
    "*Overfitting*:\n",
    "- When a model captures not just the underlying trend in the data, but also the noise. It performs very well on the training data but poorly on unseen data (like test data).\n",
    "\n",
    "*Underfitting*:\n",
    "- When a model is too simple to capture the underlying trend in the data. It performs poorly both on the training data and unseen data.\n",
    "\n",
    "*Consequences*:\n",
    "- Overfitting: Good performance on training data but poor generalization to new, unseen data.\n",
    "- Underfitting: Poor performance on both training and test data.\n",
    "\n",
    "*Mitigation*:\n",
    "- Overfitting: Use simpler models, apply regularization, use more data, or apply techniques like cross-validation.\n",
    "- Underfitting: Use more complex models, add more features, or decrease regularization.\n",
    "\n",
    "**Q2: How can we reduce overfitting?**\n",
    "\n",
    "- Use more data: Increasing the training set can help the model generalize better.\n",
    "- Simplify the model: Use fewer parameters or features.\n",
    "- Use cross-validation: Helps to get a better estimate of model performance on unseen data.\n",
    "- Apply regularization: Techniques like L1 (Lasso) and L2 (Ridge) add penalty terms that prevent coefficients from becoming too large.\n",
    "- Prune decision trees: Remove branches that have little power to predict.\n",
    "- Dropouts in neural networks: Randomly set a fraction of input units to 0 at each update during training time.\n",
    "\n",
    "**Q3: Explain underfitting. List scenarios where underfitting can occur in ML.**\n",
    "\n",
    "*Underfitting*:\n",
    "- It occurs when the model fails to capture the underlying trend of the data. \n",
    "*Scenarios*:\n",
    "- Using a linear model for non-linear data.\n",
    "- Having too few features in the training set.\n",
    "- Using a small neural network for a complex problem.\n",
    "- Overly simplifying any algorithm.\n",
    "\n",
    "**Q4: Explain the bias-variance tradeoff in machine learning.**\n",
    "\n",
    "- **Bias**: Error due to overly simplistic assumptions in the learning algorithm. High bias can cause the model to miss relevant relations between features and target outputs.\n",
    "- **Variance**: Error due to too much complexity in the learning algorithm. High variance can cause overfitting.\n",
    "\n",
    "*Relationship*:\n",
    "- Increasing a model's complexity might decrease bias but increase variance, and vice-versa.\n",
    "- Ideally, one aims for a good balance between bias and variance, ensuring minimal total error.\n",
    "\n",
    "**Q5: Common methods for detecting overfitting and underfitting.**\n",
    "\n",
    "- **Training vs. Test Error**: A model that overfits will have a low training error but a high test error. Underfitting will manifest as a high training error.\n",
    "- **Cross-Validation**: Consistently high validation errors across different data subsets can indicate overfitting.\n",
    "- **Learning Curves**: Plotting training and validation errors over increasing amounts of data can indicate where the model begins to overfit.\n",
    "\n",
    "**Q6: Compare and contrast bias and variance in machine learning.**\n",
    "\n",
    "- **Bias**: \n",
    "  - *Description*: The model's assumptions are wrong about the target function.\n",
    "  - *High Bias Example*: Assuming data is linear when it has a non-linear relationship.\n",
    "  - *Performance*: Tends to have high error on training and test data.\n",
    "  \n",
    "- **Variance**:\n",
    "  - *Description*: The model is highly sensitive to fluctuations in the training data.\n",
    "  - *High Variance Example*: A high-degree polynomial regression model on a simple dataset.\n",
    "  - *Performance*: Performs well on training data but poorly on test data.\n",
    "\n",
    "**Q7: What is regularization in machine learning?**\n",
    "\n",
    "- **Regularization**: A technique to add a penalty on the complexity of the model, preventing overfitting.\n",
    "- **Common techniques**:\n",
    "  - **L1 Regularization (Lasso)**: Adds a penalty equal to the absolute value of the magnitude of coefficients. Can reduce some coefficients to zero, effectively performing feature selection.\n",
    "  - **L2 Regularization (Ridge)**: Adds a penalty proportional to the square of the magnitude of coefficients. It will make coefficients smaller but not zero.\n",
    "  - **Elastic Net**: A combination of L1 and L2 regularization.\n",
    "\n",
    "By adding these penalties, regularization ensures that the model remains simpler and more robust, leading to better generalization on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
