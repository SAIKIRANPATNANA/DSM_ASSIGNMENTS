{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is hierarchical clustering, and how is it different from other clustering techniques?**\n",
    "\n",
    "*Answer*: \n",
    "Hierarchical clustering is a method of clustering that builds a hierarchy of clusters by either continually splitting or merging groups. Unlike partitioning methods like K-means which divides the dataset into non-overlapping clusters, hierarchical clustering provides a tree-like structure (dendrogram) that represents the nested clusters. This method does not require the number of clusters to be specified a priori.\n",
    "\n",
    "**Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief.**\n",
    "\n",
    "*Answer*: \n",
    "- **Agglomerative** (bottom-up): Starts by treating each data point as a single cluster, and then continually merges the closest pairs of clusters until only one cluster remains.\n",
    "- **Divisive** (top-down): Starts with all data points as a single cluster and then recursively splits the clusters into smaller clusters, until each data point is its own cluster.\n",
    "\n",
    "**Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the common distance metrics used?**\n",
    "\n",
    "*Answer*: \n",
    "The distance between two clusters is determined by linkage criteria:\n",
    "- **Single Linkage**: Distance between the two closest points in different clusters.\n",
    "- **Complete Linkage**: Distance between the two farthest points in different clusters.\n",
    "- **Average Linkage**: Average distance between all pairs of points in different clusters.\n",
    "- **Centroid Linkage**: Distance between the centroids of two clusters.\n",
    "\n",
    "Common distance metrics include Euclidean, Manhattan, and Mahalanobis.\n",
    "\n",
    "**Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some common methods used for this purpose?**\n",
    "\n",
    "*Answer*: \n",
    "Examining the dendrogram is a common way. By looking at where the largest jumps in linkage distances occur, one can choose an appropriate \"cut\" in the dendrogram to define the number of clusters. Another approach is the inconsistency coefficient, which measures the relative inconsistency of every link height compared to the average height of links below it.\n",
    "\n",
    "**Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?**\n",
    "\n",
    "*Answer*: \n",
    "Dendrograms are tree-like diagrams that represent the merging (or splitting) process of hierarchical clustering. Each leaf represents a data point and branches show the sequence of merges or splits. By examining the dendrogram, one can understand the hierarchy of clusters and decide on an appropriate cut-off to determine the number of clusters.\n",
    "\n",
    "**Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the distance metrics different for each type of data?**\n",
    "\n",
    "*Answer*: \n",
    "Yes, hierarchical clustering can be used for both types of data. For numerical data, distance metrics like Euclidean, Manhattan, or Mahalanobis are common. For categorical data, specialized metrics like the Jaccard similarity coefficient or Hamming distance are more appropriate.\n",
    "\n",
    "**Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?**\n",
    "\n",
    "*Answer*: \n",
    "In the dendrogram of a hierarchical clustering, outliers or anomalies may appear as data points or small clusters that merge very late (at a relatively high distance) compared to the majority of the data. These points are typically far from the main clusters and can be considered as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
