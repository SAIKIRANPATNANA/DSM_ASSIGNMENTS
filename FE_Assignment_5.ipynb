{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Min-Max Scaling**\n",
    "\n",
    "- Min-Max scaling is a technique used in data preprocessing to scale numerical features to a fixed range, usually between 0 and 1.\n",
    "- It is calculated using the formula:\n",
    "\n",
    "  \\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "  Where:\n",
    "  - \\( X \\) is the original feature value.\n",
    "  - \\( X_{\\text{min}} \\) is the minimum value of the feature in the dataset.\n",
    "  - \\( X_{\\text{max}} \\) is the maximum value of the feature in the dataset.\n",
    "- Min-Max scaling preserves the relationship between data points while bringing them to a common scale.\n",
    "\n",
    "**Example:**\n",
    "Suppose we have a dataset with a feature \"Income\" ranging from \\$20,000 to \\$100,000. We can apply Min-Max scaling to scale these values to a range between 0 and 1, making the income values comparable regardless of their original scale.\n",
    "\n",
    "**Q2: Unit Vector Technique**\n",
    "\n",
    "- The unit vector technique scales each feature to have unit norm (length 1).\n",
    "- It differs from Min-Max scaling in that it doesn't necessarily bring the features within a fixed range like 0 to 1; instead, it ensures that each feature vector has a length of 1.\n",
    "- It's calculated as:\n",
    "\n",
    "  \\[ X_{\\text{scaled}} = \\frac{X}{||X||} \\]\n",
    "\n",
    "  Where \\( ||X|| \\) represents the Euclidean norm or length of the feature vector \\( X \\).\n",
    "\n",
    "**Example:**\n",
    "Consider a dataset with features representing different physical quantities like height, weight, and age. By using the unit vector technique, we normalize each feature vector so that its length becomes 1, making them comparable in terms of their direction in the feature space.\n",
    "\n",
    "Let's move on to Q3.\n",
    "\n",
    "**Q3: Principle Component Analysis (PCA)**\n",
    "\n",
    "- PCA is a dimensionality reduction technique used to reduce the number of features in a dataset while preserving most of the variability in the data.\n",
    "- It works by transforming the original features into a new set of orthogonal components called principal components (PCs), which are linear combinations of the original features.\n",
    "- PCA identifies the directions (principal components) that maximize the variance in the data, allowing us to retain the most important information while reducing the dimensionality.\n",
    "- It is commonly used in data preprocessing and feature extraction.\n",
    "\n",
    "**Example:**\n",
    "Suppose we have a dataset with multiple correlated features such as height, weight, and body mass index (BMI). By applying PCA, we can transform these correlated features into a smaller set of uncorrelated principal components, capturing most of the variability in the data. This can help in reducing computational complexity and multicollinearity issues in subsequent analysis.\n",
    "\n",
    "Let's proceed to Q4.\n",
    "\n",
    "**Q4: Relationship between PCA and Feature Extraction**\n",
    "\n",
    "- PCA can be used for feature extraction by transforming the original features into a smaller set of principal components (PCs) that capture most of the variability in the data.\n",
    "- Feature extraction aims to reduce the dimensionality of the dataset by identifying a smaller set of features that best represent the data.\n",
    "- PCA identifies the directions of maximum variance in the data and projects the original features onto these directions to create new features (principal components).\n",
    "- The principal components are ordered by the amount of variance they explain, allowing us to choose a subset of the most important components to retain as features for further analysis.\n",
    "\n",
    "**Example:**\n",
    "Suppose we have a dataset with high-dimensional features representing images of handwritten digits. By applying PCA, we can extract a smaller set of principal components that capture the essential patterns and variations in the images. These principal components can then be used as features for training a machine learning model to classify the digits.\n",
    "\n",
    "Let's move on to Q5.\n",
    "\n",
    "**Q5: Using Min-Max Scaling for Food Delivery Service Data**\n",
    "\n",
    "To preprocess the data for building a recommendation system for a food delivery service, we can use Min-Max scaling as follows:\n",
    "\n",
    "1. Identify the numerical features in the dataset, such as price, rating, and delivery time.\n",
    "2. Calculate the minimum and maximum values for each numerical feature.\n",
    "3. Apply Min-Max scaling to each numerical feature using the formula:\n",
    "\n",
    "   \\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "   Where \\( X \\) is the original feature value, \\( X_{\\text{min}} \\) is the minimum value of the feature, and \\( X_{\\text{max}} \\) is the maximum value of the feature.\n",
    "\n",
    "4. The scaled values will now be in the range of 0 to 1, making them suitable for use in the recommendation system.\n",
    "\n",
    "For example, if the original price of a food item ranges from $5 to $20, Min-Max scaling will transform these values to a range of 0 to 1, making them comparable with other scaled features.\n",
    "\n",
    "Let's proceed to Q6.\n",
    "\n",
    "**Q6: Using PCA for Stock Price Prediction**\n",
    "\n",
    "To reduce the dimensionality of the dataset containing various features for predicting stock prices, we can use PCA as follows:\n",
    "\n",
    "1. Identify the features in the dataset, such as company financial data and market trends.\n",
    "2. Standardize the numerical features to have a mean of 0 and a standard deviation of 1 to ensure they contribute equally to PCA.\n",
    "3. Apply PCA to the standardized dataset to transform the original features into a set of principal components.\n",
    "4. Determine the number of principal components to retain based on the cumulative explained variance ratio, ensuring it captures a significant portion of the variability in the data.\n",
    "5. Project the original dataset onto the selected principal components to obtain a reduced-dimensional representation of the data.\n",
    "6. Use the reduced dataset with fewer dimensions for training the stock price prediction model.\n",
    "\n",
    "PCA helps in reducing the dimensionality of the dataset while preserving most of its variability, making it computationally efficient and potentially improving the performance of the prediction model.\n",
    "\n",
    "Moving on to Q7.\n",
    "\n",
    "**Q7: Min-Max Scaling for Dataset**\n",
    "\n",
    "Given the dataset: [1, 5, 10, 15, 20], we'll perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "1. Calculate the minimum and maximum values of the dataset:\n",
    "   - \\( X_{\\text{min}} = 1 \\)\n",
    "   - \\( X_{\\text{max}} = 20 \\)\n",
    "\n",
    "2. Apply Min-Max scaling to each value using the formula:\n",
    "\n",
    "   \\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\times ( \\text{new_max} - \\text{new_min}) + \\text{new_min} \\]\n",
    "\n",
    "   Here, \\( \\text{new_min} = -1 \\) and \\( \\text{new_max} = 1 \\).\n",
    "\n",
    "3. Perform Min-Max scaling for each value:\n",
    "   - For 1: \\( X_{\\text{scaled}} = \\frac{1 - 1}{20 - 1} \\times (1 - (-1)) + (-1) = -1 \\)\n",
    "   - For 5: \\( X_{\\text{scaled}} = \\frac{5 - 1}{20 - 1} \\times (1 - (-1)) + (-1) \\approx -0.5 \\)\n",
    "   - For 10: \\( X_{\\text{scaled}} = \\frac{10 - 1}{20 - 1} \\times (1 - (-1)) + (-1) \\approx 0 \\)\n",
    "   - For 15: \\( X_{\\text{scaled}} = \\frac{15 - 1}{20 - 1} \\times (1 - (-1)) + (-1) \\approx 0.5 \\)\n",
    "   - For 20: \\( X_{\\text{scaled}} = \\frac{20 - 1}{20 - 1} \\times (1 - (-1)) + (-1) = 1 \\)\n",
    "\n",
    "So, the Min-Max scaled values for the dataset [1, 5, 10, 15, 20] within the range of -1 to 1 are approximately [-1, -0.5, 0, 0.5, 1].\n",
    "\n",
    "Let's proceed to Q8.\n",
    "\n",
    "**Q8: Feature Extraction using PCA**\n",
    "\n",
    "For the dataset containing features [height, weight, age, gender, blood pressure], we'll perform feature extraction using PCA.\n",
    "\n",
    "1. Standardize the numerical features (height, weight, age, blood pressure) to have a mean of 0 and a standard deviation of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
