{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?**\n",
    "*Answer*: \n",
    "A contingency matrix, often known as a confusion matrix in classification, is a table used to evaluate the performance of an algorithm. Each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class. It provides a visualization of the true positives, false positives, true negatives, and false negatives, and helps in deriving metrics like precision, recall, and accuracy.\n",
    "\n",
    "**Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations?**\n",
    "*Answer*: \n",
    "A pair confusion matrix is used in clustering to determine the pairs of samples that were clustered correctly or incorrectly. It contains the following values: the number of pairs of points that are in the same cluster in the true and predicted labels, the number of pairs in the same cluster for the true labels but in different clusters for the predicted labels, and so on. It is especially useful in situations when you want to evaluate clustering algorithms, as opposed to classification algorithms.\n",
    "\n",
    "**Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models?**\n",
    "*Answer*: \n",
    "An extrinsic measure evaluates a model based on its performance in a real-world task. For instance, in natural language processing, an extrinsic measure for a language model might be its effectiveness in a machine translation task or a speech recognition system. It gives a more practical sense of how improvements in the model will translate to improvements in real applications.\n",
    "\n",
    "**Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?**\n",
    "*Answer*: \n",
    "An intrinsic measure evaluates a model based on its performance in the task it was specifically designed for, without considering its utility in an application. It differs from extrinsic measures which evaluate models based on their performance in real-world tasks. In the context of a language model, an intrinsic measure might be the model's perplexity, while an extrinsic measure could be its translation accuracy in a machine translation system.\n",
    "\n",
    "**Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify strengths and weaknesses of a model?**\n",
    "*Answer*: \n",
    "The confusion matrix provides a summary of the prediction results for a classification problem. It can show the correct and incorrect predictions for each class, helping to identify if the model is confusing two classes more frequently than others. By deriving metrics like precision, recall, F1-score from the matrix, we can determine the model's strengths and weaknesses in terms of false positives, false negatives, etc.\n",
    "\n",
    "**Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms, and how can they be interpreted?**\n",
    "*Answer*: \n",
    "Common intrinsic measures for unsupervised learning include:\n",
    "- **Silhouette Coefficient**: Measures the similarity of an object to its cluster compared to other clusters. The values range from -1 to 1, where a high value indicates correct clustering.\n",
    "- **Davies-Bouldin Index**: Measures the average similarity ratio of each cluster with its most similar cluster. Lower values indicate better clustering.\n",
    "- **Calinski-Harabasz Index**: Measures the ratio of between-cluster dispersion to within-cluster dispersion. Higher values indicate better clustering.\n",
    "These metrics help in determining how well data points are clustered, with respect to cohesion within clusters and separation between clusters.\n",
    "\n",
    "**Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and how can these limitations be addressed?**\n",
    "*Answer*: \n",
    "Accuracy might not be a suitable metric, especially if:\n",
    "- The dataset is imbalanced. A model predicting only the majority class can still have high accuracy.\n",
    "- The costs of false positives and false negatives are different. For instance, in medical diagnoses, a false negative might have more severe consequences than a false positive.\n",
    "These limitations can be addressed by using metrics like precision, recall, F1-score, and the area under the ROC curve (AUC-ROC) which provide a more comprehensive view of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
