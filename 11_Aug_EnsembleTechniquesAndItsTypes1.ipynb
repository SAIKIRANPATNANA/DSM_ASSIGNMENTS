{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is an ensemble technique in machine learning?**\n",
    "\n",
    "*Answer*: Ensemble techniques in machine learning involve combining multiple models to improve the overall performance of the prediction, compared to using a single model. It's essentially a method to produce a single optimal predictive model by utilizing a set of predictions from multiple models.\n",
    "\n",
    "**Q2. Why are ensemble techniques used in machine learning?**\n",
    "\n",
    "*Answer*: Ensemble techniques are used in machine learning to:\n",
    "1. Increase accuracy by combining predictions from various models.\n",
    "2. Reduce overfitting by averaging out biases.\n",
    "3. Improve the robustness of predictions.\n",
    "4. Address the limitations of individual models.\n",
    "\n",
    "**Q3. What is bagging?**\n",
    "\n",
    "*Answer*: Bagging, or Bootstrap Aggregating, is an ensemble technique where multiple instances of a dataset are created by random resampling (with replacement) of the original dataset. A model is then trained on each of these instances. For predictions, all the models vote (in the case of classification) or the results are averaged (in the case of regression).\n",
    "\n",
    "**Q4. What is boosting?**\n",
    "\n",
    "*Answer*: Boosting is an ensemble technique that focuses on training models sequentially. Each new model corrects the errors of its predecessor. It aims to convert weak models (models that only slightly perform better than random guessing) into strong ones by weighting the instances that are hard to classify and focusing on them in subsequent models.\n",
    "\n",
    "**Q5. What are the benefits of using ensemble techniques?**\n",
    "\n",
    "*Answer*: Benefits of using ensemble techniques include:\n",
    "1. Enhanced accuracy and performance.\n",
    "2. Reduction in model variance, making predictions more consistent.\n",
    "3. Protection against overfitting.\n",
    "4. Better handling of unbalanced datasets.\n",
    "\n",
    "**Q6. Are ensemble techniques always better than individual models?**\n",
    "\n",
    "*Answer*: Not always. While ensemble techniques often outperform individual models, there are situations where the computational cost, complexity, and training time may not justify the marginal improvement in accuracy. The suitability of ensemble methods depends on the specific problem and the data at hand.\n",
    "\n",
    "**Q7. How is the confidence interval calculated using bootstrap?**\n",
    "\n",
    "*Answer*: In bootstrapping, a large number of resamples (with replacement) are taken from the original dataset, and the statistic of interest (mean, median, etc.) is calculated for each resample. The confidence interval is then estimated from the distribution of these bootstrapped statistics, typically by taking percentiles. For a 95% confidence interval, it's common to use the 2.5th and 97.5th percentiles.\n",
    "\n",
    "**Q8. How does bootstrap work and What are the steps involved in bootstrap?**\n",
    "\n",
    "*Answer*: \n",
    "1. Draw a sample from the original dataset with replacement. This sample should be of the same size as the original dataset.\n",
    "2. Calculate the desired statistic (e.g., mean, median) on this sample.\n",
    "3. Repeat the above two steps a large number of times (e.g., 1000 or 10,000 times).\n",
    "4. The distribution of the calculated statistic across all bootstrap samples forms the bootstrap distribution.\n",
    "5. Estimate confidence intervals and other statistics using this bootstrap distribution.\n",
    "\n",
    "**Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height.**\n",
    "\n",
    "*Answer*: To estimate the 95% confidence interval using bootstrapping:\n",
    "1. Resample (with replacement) the original set of 50 tree heights. \n",
    "2. Calculate the mean of this resampled set.\n",
    "3. Repeat the above two steps many times (e.g., 10,000 times) to get a distribution of means.\n",
    "4. From this distribution, the 2.5th percentile and 97.5th percentile values will provide the 95% confidence interval.\n",
    "\n",
    "Using Python, it might look something like this:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Original data\n",
    "data = np.random.normal(15, 2, 50)  # assuming a normal distribution for simplicity\n",
    "bootstrap_means = []\n",
    "\n",
    "# Bootstrap resampling\n",
    "for _ in range(10000):\n",
    "    sample = np.random.choice(data, size=len(data), replace=True)\n",
    "    bootstrap_means.append(sample.mean())\n",
    "\n",
    "# 95% confidence interval\n",
    "conf_interval = (np.percentile(bootstrap_means, 2.5), np.percentile(bootstrap_means, 97.5))\n",
    "```\n",
    "\n",
    "The variable `conf_interval` would then hold the lower and upper bounds of the 95% confidence interval for the mean height of the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
