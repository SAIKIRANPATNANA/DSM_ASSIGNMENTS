{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?**\n",
    "\n",
    "*Answer*: \n",
    "The primary difference is how they measure distance:\n",
    "- **Euclidean distance** (L2 norm) is the straight-line distance between two points in a space, computed as \\( \\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2} \\) for 2D space.\n",
    "- **Manhattan distance** (L1 norm) computes the distance between two points as the sum of the absolute differences of their coordinates, calculated as \\( |x_2-x_1| + |y_2-y_1| \\) for 2D space.\n",
    "\n",
    "The choice of distance metric can influence the KNN's decision boundary. Manhattan distance might be more robust to outliers in some cases, while Euclidean might be more appropriate for data distributed in circular or spherical patterns.\n",
    "\n",
    "**Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?**\n",
    "\n",
    "*Answer*: \n",
    "The optimal value of \\( k \\) can be found using techniques like:\n",
    "- **Cross-validation**: By splitting the data into training and validation sets and testing the performance of the KNN model on the validation set for various values of \\( k \\). The value of \\( k \\) that provides the best performance on the validation set is selected.\n",
    "- **Elbow Method**: Plotting the error rate vs. values of \\( k \\) and choosing the \"elbow\" point, where the error rate starts to stabilize.\n",
    "\n",
    "**Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?**\n",
    "\n",
    "*Answer*: \n",
    "The choice of distance metric impacts the way the algorithm perceives the proximity of instances. For instance:\n",
    "- **Manhattan Distance** might be preferable when the data has a grid-like structure or when the difference along individual axes has specific importance.\n",
    "- **Euclidean Distance** is generally suitable when the data is isotropic and doesn't have a pronounced grid pattern.\n",
    "\n",
    "The dataset's characteristics and the problem context should guide the choice of distance metric.\n",
    "\n",
    "**Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?**\n",
    "\n",
    "*Answer*: \n",
    "Common hyperparameters include:\n",
    "- **Number of Neighbors (\\( k \\))**: Affects the model's sensitivity to noise in the data.\n",
    "- **Distance Metric**: e.g., Euclidean, Manhattan, Minkowski, etc.\n",
    "- **Weights**: Uniform (all points in the neighborhood are weighted equally) or distance-based (closer points have more influence).\n",
    "\n",
    "Hyperparameters can be tuned using techniques like grid search combined with cross-validation to evaluate different combinations and choose the best-performing set.\n",
    "\n",
    "**Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?**\n",
    "\n",
    "*Answer*: \n",
    "KNN's performance generally improves with a larger training set, as having more data points can lead to more accurate neighborhood determinations. However, it can also make the algorithm slower. Techniques like **random sampling** or **active learning** can be used to optimize the size and quality of the training set.\n",
    "\n",
    "**Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?**\n",
    "\n",
    "*Answer*: \n",
    "Potential drawbacks:\n",
    "- **Computational Complexity**: KNN has to compute distances to every point in the training dataset for each prediction.\n",
    "- **Memory Intensive**: Requires storing the entire dataset.\n",
    "- **Sensitivity to Irrelevant Features**: KNN relies on distances, so irrelevant features can affect performance.\n",
    "\n",
    "To overcome:\n",
    "- Use **dimensionality reduction** techniques like PCA to reduce irrelevant features.\n",
    "- Implement **distance indexing structures** like KD-trees or Ball trees for faster nearest neighbor search.\n",
    "- Feature scaling and normalization to ensure all features contribute equally to the distance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
