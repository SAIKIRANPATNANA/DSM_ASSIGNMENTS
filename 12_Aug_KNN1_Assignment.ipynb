{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is the KNN algorithm?**\n",
    "\n",
    "*Answer*: KNN stands for K-Nearest Neighbors. It is a non-parametric, lazy learning algorithm. For classification, it works by finding the \\( k \\) training samples closest in distance to the input point and returning the mode of their labels as the predicted label. For regression, it returns the mean or median of the values of the \\( k \\) nearest neighbors.\n",
    "\n",
    "**Q2. How do you choose the value of K in KNN?**\n",
    "\n",
    "*Answer*: The value of \\( k \\) in KNN is typically chosen using cross-validation. A small value of \\( k \\) is noise-sensitive and is susceptible to overfitting, while a large value is computationally expensive and might include neighbors from other classes. A common technique is to test multiple values for \\( k \\) and select the one that has the best performance on a validation set.\n",
    "\n",
    "**Q3. What is the difference between KNN classifier and KNN regressor?**\n",
    "\n",
    "*Answer*: The KNN classifier is used for classification tasks. It assigns a new input to the class that has the majority among its \\( k \\) nearest neighbors. The KNN regressor, on the other hand, is used for regression tasks. It assigns a new input a value, which is typically the average (or median) of the values of its \\( k \\) nearest neighbors.\n",
    "\n",
    "**Q4. How do you measure the performance of KNN?**\n",
    "\n",
    "*Answer*: For classification tasks, metrics like accuracy, precision, recall, and F1-score can be used. For regression tasks, metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) are used.\n",
    "\n",
    "**Q5. What is the curse of dimensionality in KNN?**\n",
    "\n",
    "*Answer*: The curse of dimensionality refers to the problem that arises when dealing with high-dimensional data. In high dimensions, the distance between most pairs of points tends to be similar, making it difficult for KNN to find meaningful nearest neighbors. This can lead to reduced model performance and increased computational cost.\n",
    "\n",
    "**Q6. How do you handle missing values in KNN?**\n",
    "\n",
    "*Answer*: Missing values can be handled in various ways before using KNN:\n",
    "1. Imputation: Replace missing values with mean, median, or mode of that feature.\n",
    "2. Deletion: Remove instances with missing values.\n",
    "3. Weighted KNN: Modify the distance metric to handle missing values.\n",
    "\n",
    "**Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?**\n",
    "\n",
    "*Answer*: The performance of KNN classifier vs. regressor is task-dependent. The KNN classifier is suitable for classification problems where the goal is to assign discrete labels to instances. In contrast, the KNN regressor is suitable for regression problems where the goal is to predict a continuous value. The efficacy of each depends on the nature and distribution of the data.\n",
    "\n",
    "**Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?**\n",
    "\n",
    "*Strengths*:\n",
    "- Simple and intuitive.\n",
    "- Non-parametric, making it flexible.\n",
    "- Can be used for both classification and regression.\n",
    "\n",
    "*Weaknesses*:\n",
    "- Computationally expensive, especially with a large dataset.\n",
    "- Sensitive to irrelevant or redundant features.\n",
    "- Suffers from the curse of dimensionality.\n",
    "- Sensitive to noise in the dataset.\n",
    "\n",
    "Addressing Weaknesses:\n",
    "- Use dimensionality reduction techniques.\n",
    "- Feature selection or engineering to reduce noise.\n",
    "- Use algorithms like KD-tree or Ball tree for faster nearest neighbor search.\n",
    "\n",
    "**Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?**\n",
    "\n",
    "*Answer*: Euclidean distance is the straight-line distance between two points in Euclidean space. It's calculated as \\( \\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2} \\). Manhattan distance, or L1 distance, is the distance between two points measured along the axes at right angles. It's calculated as \\( |x_2-x_1| + |y_2-y_1| \\).\n",
    "\n",
    "**Q10. What is the role of feature scaling in KNN?**\n",
    "\n",
    "*Answer*: Feature scaling is crucial in KNN because it uses distance metrics to determine the nearest neighbors. If one feature has a broad range of values, it can dominate other features when calculating the distance, leading to sub-optimal performance. Scaling ensures all features contribute equally to the distance calculation. Common methods include Min-Max scaling and Z-score normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
